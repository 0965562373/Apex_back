{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPJ348yT0n46qdGum6NvBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abeni-hub/Apex_back/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Xel3XuA3609J"
      },
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "SRjODise7Kjy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are few other commands that can be used to iteract and get information about the environment\n"
      ],
      "metadata": {
        "id": "C2bER_Dt7l4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n) # get number of states\n",
        "print(env.action_space.n) # get number of actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UvwYzYM7wPH",
        "outputId": "d9ff4353-2ab1-47af-c4cb-e00e396b4fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # reset environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZqQgDBOFesi",
        "outputId": "3e787b00-efd2-4092-9c43-562f51eefbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample() # get random action"
      ],
      "metadata": {
        "id": "PENwdXC5Fm2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_state, reward, done, info = env.step(action) # take action , notice it returns information about the action"
      ],
      "metadata": {
        "id": "LKbVyX5hFsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "id": "Xc2vpgcZGGXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frozen Lake Environment\n",
        "Building the Q-Table"
      ],
      "metadata": {
        "id": "cLPgE3JiGT9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n"
      ],
      "metadata": {
        "id": "W-R-jo3DGXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros((STATES, ACTIONS)) # create a matrix with all 0 values\n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfANU2a0IIkJ",
        "outputId": "9a625c99-d515-4191-e260-73b33a01ee6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constant"
      ],
      "metadata": {
        "id": "ezrLK8tYIZDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "EPISODES = 2000 # how many times to run the environment from the beginning\n",
        "MAX_STEPS = 100 # max number of steps allowed for each run of enviroment\n",
        "\n",
        "LEARNING_RATE = 0.81 # learning rate\n",
        "GAMMA = 0.96 # discount factor"
      ],
      "metadata": {
        "id": "n9StLJD7Ia5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picking an Action"
      ],
      "metadata": {
        "id": "JhwWbxSrJAWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.9 # start with a 90 % chance of picking a random action\n",
        "state = env.reset() # reset environment\n",
        "# code to pick action\n",
        "if np.random.uniform(0, 1) < epsilon: # we will check if a randomly selected value is less than epsilon.\n",
        "    action = env.action_space.sample() # take random action\n",
        "else:\n",
        "    action = np.argmax(Q[state, :]) # use Q table to pick best action based on current values"
      ],
      "metadata": {
        "id": "KXCOY1rHJDPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating Q Values"
      ],
      "metadata": {
        "id": "OwNGia-ANDDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q[state,action] = Q[state,action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])"
      ],
      "metadata": {
        "id": "Jto9AWvkNFv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n\n",
        "\n",
        "Q = np.zeros((STATES, ACTIONS))\n",
        "\n",
        "EPISODES = 1500 # how many times to run the environment from the beginning\n",
        "MAX_STEPS = 100 # max number of steps allowed for each run of enviroment\n",
        "\n",
        "LEARNING_RATE = 0.81 # learning rate\n",
        "GAMMA = 0.96 # discount factor\n",
        "\n",
        "epsilon = 0.9"
      ],
      "metadata": {
        "id": "22CKKX3fNcu5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "RENDER = True\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    for _ in range(MAX_STEPS):\n",
        "\n",
        "      if RENDER:\n",
        "        env.render()\n",
        "\n",
        "      if np.random.uniform(0,1) < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = np.argmax(Q[state, :])\n",
        "\n",
        "      next_state, reward , done, _ = env.step(action)\n",
        "\n",
        "      Q[state,action] = Q[state,action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(reward)\n",
        "        epsilon-=0.001\n",
        "        break # reached goal\n",
        "\n",
        "print(Q)\n",
        "print(f\"Average reward: {sum(rewards)/len(rewards)}:\")\n",
        "# and know we can see our Q Values!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy1-5ET3OCgl",
        "outputId": "aa760efa-b5ac-46c2-bb8a-9169191ca772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the training progress and see how the agent improved"
      ],
      "metadata": {
        "id": "iX1X8h2CR1Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rewards)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wrsh4vRmR8H3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}